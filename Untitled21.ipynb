{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "BcDRm2umknFk",
        "outputId": "08941112-9049-4a9a-93b5-4d30fb78385f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "edf16356fa8243988ca08c2072568167"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6V2Rs1CwZ5jN",
        "outputId": "f55f4fef-13fd-49f7-adbc-d6cfd0e80051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Word2Vec model...\n",
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "Test Accuracy: 0.9453\n",
            "spam\n",
            "ham\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (uncomment and run once if needed)\n",
        "# !pip install pandas nltk scikit-learn gensim\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import gensim.downloader as api\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Download stopwords (no punkt needed anymore)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/ham.csv', encoding='latin-1')[['v1', 'v2']]\n",
        "df.columns = ['Label', 'Message']\n",
        "\n",
        "# Preprocessing function using RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = tokenizer.tokenize(text.lower())\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "df['Tokens'] = df['Message'].apply(preprocess)\n",
        "\n",
        "# Load Google News Word2Vec model\n",
        "print(\"Loading Word2Vec model...\")\n",
        "w2v_model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "# Convert each message into a fixed-length vector\n",
        "def vectorize(tokens, model, size=300):\n",
        "    valid_words = [word for word in tokens if word in model]\n",
        "    if not valid_words:\n",
        "        return np.zeros(size)\n",
        "    return np.mean([model[word] for word in valid_words], axis=0)\n",
        "\n",
        "df['Vector'] = df['Tokens'].apply(lambda x: vectorize(x, w2v_model))\n",
        "X = np.vstack(df['Vector'].values)\n",
        "y = df['Label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train logistic regression model\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and print accuracy\n",
        "y_pred = clf.predict(X_test)\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "# Prediction function\n",
        "def predict_message_class(model, w2v_model, message):\n",
        "    tokens = preprocess(message)\n",
        "    vector = vectorize(tokens, w2v_model).reshape(1, -1)\n",
        "    prediction = model.predict(vector)[0]\n",
        "    return 'spam' if prediction == 1 else 'ham'\n",
        "\n",
        "# Test predictions\n",
        "print(predict_message_class(clf, w2v_model, \"Congratulations! You've won a free ticket. Reply YES to claim.\"))\n",
        "print(predict_message_class(clf, w2v_model, \"Are we still meeting today at 5?\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Uncomment to install required packages\n",
        "# !pip install pandas nltk scikit-learn gensim\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import gensim.downloader as api\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 📥 Download stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# 📂 Load dataset\n",
        "df = pd.read_csv('twitter.csv')  # Update the path as needed\n",
        "df.columns = df.columns.str.strip()  # Clean column names\n",
        "df = df[['text', 'airline_sentiment']].dropna()\n",
        "\n",
        "# 🧼 Preprocessing using RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = tokenizer.tokenize(text.lower())\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "df['Tokens'] = df['text'].apply(preprocess)\n",
        "\n",
        "# 🌐 Load Word2Vec model\n",
        "print(\"Loading Word2Vec model...\")\n",
        "w2v_model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "# 🔢 Vectorize each tweet\n",
        "def vectorize(tokens, model, size=300):\n",
        "    valid_words = [word for word in tokens if word in model]\n",
        "    if not valid_words:\n",
        "        return np.zeros(size)\n",
        "    return np.mean([model[word] for word in valid_words], axis=0)\n",
        "\n",
        "df['Vector'] = df['Tokens'].apply(lambda x: vectorize(x, w2v_model))\n",
        "\n",
        "# 🎯 Prepare features and labels\n",
        "X = np.vstack(df['Vector'].values)\n",
        "y = df['airline_sentiment']  # Sentiment labels: typically 'positive', 'neutral', 'negative'\n",
        "\n",
        "# 🧪 Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 🏋️ Train logistic regression model\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# ✅ Evaluate model\n",
        "y_pred = clf.predict(X_test)\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "# 🔍 Prediction function\n",
        "def predict_tweet_sentiment(model, w2v_model, message):\n",
        "    tokens = preprocess(message)\n",
        "    vector = vectorize(tokens, w2v_model).reshape(1, -1)\n",
        "    return model.predict(vector)[0]\n",
        "\n",
        "# 💬 Test predictions\n",
        "print(predict_tweet_sentiment(clf, w2v_model, \"This airline is the worst. Never flying again!\"))\n",
        "print(predict_tweet_sentiment(clf, w2v_model, \"Really happy with the flight service today.\"))\n"
      ],
      "metadata": {
        "id": "ngolEo60j0vW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faea2ba6-e15d-41e2-f338-9cdb154944a7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Word2Vec model...\n",
            "Test Accuracy: 0.7804\n",
            "negative\n",
            "positive\n"
          ]
        }
      ]
    }
  ]
}