{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWngLFhn72Af"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install datasets transformers torch streamlit\n",
        "\n",
        "# Importing necessary libraries\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, pipeline\n",
        "import torch\n",
        "import math\n",
        "from torch.utils.data import DataLoader\n",
        "import streamlit as st\n",
        "\n",
        "# Step 1: Data Loading & Tokenization\n",
        "\n",
        "# Load the WikiText-2 dataset\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "\n",
        "# Initialize the GPT-2 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Tokenization function to prepare text for GPT-2\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "# Apply the tokenizer to the dataset\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Step 2: Model Selection\n",
        "\n",
        "# Load pre-trained GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Step 3: Fine-Tuning the Model\n",
        "\n",
        "# Define the training arguments for fine-tuning\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",            # Directory to store results\n",
        "    num_train_epochs=3,                # Number of training epochs\n",
        "    per_device_train_batch_size=8,     # Batch size for training\n",
        "    per_device_eval_batch_size=8,      # Batch size for evaluation\n",
        "    logging_dir=\"./logs\",              # Directory for storing logs\n",
        "    evaluation_strategy=\"epoch\",       # Evaluate model at the end of each epoch\n",
        "    save_strategy=\"epoch\",             # Save model every epoch\n",
        ")\n",
        "\n",
        "# Define the Trainer object which handles training\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Step 4: Evaluation - Perplexity and Top-K Accuracy\n",
        "\n",
        "# Perplexity calculation function\n",
        "def compute_perplexity(model, eval_dataset):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    eval_dataloader = DataLoader(eval_dataset, batch_size=8)\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in eval_dataloader:\n",
        "        outputs = model(**batch)  # Forward pass through the model\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()  # Accumulate the loss\n",
        "\n",
        "    perplexity = math.exp(total_loss / len(eval_dataloader))  # Calculate perplexity\n",
        "    return perplexity\n",
        "\n",
        "# Compute and print perplexity on the validation dataset\n",
        "perplexity = compute_perplexity(model, tokenized_datasets[\"validation\"])\n",
        "print(f\"Perplexity: {perplexity}\")\n",
        "\n",
        "# Top-K Accuracy calculation function\n",
        "def top_k_accuracy(model, eval_dataset, k=5):\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    # Initialize the text-generation pipeline\n",
        "    next_word_predictor = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "    # Evaluate top-k accuracy\n",
        "    for example in eval_dataset:\n",
        "        input_text = example[\"text\"]\n",
        "        predicted_words = next_word_predictor(input_text, max_length=50, num_return_sequences=k)\n",
        "        top_k_predictions = [prediction[\"generated_text\"].split()[-1] for prediction in predicted_words]\n",
        "\n",
        "        if input_text.split()[-1] in top_k_predictions:\n",
        "            correct_predictions += 1\n",
        "        total_predictions += 1\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions  # Calculate top-k accuracy\n",
        "    return accuracy\n",
        "\n",
        "# Compute and print top-5 accuracy on the validation dataset\n",
        "accuracy = top_k_accuracy(model, tokenized_datasets[\"validation\"], k=5)\n",
        "print(f\"Top-5 Accuracy: {accuracy}\")\n",
        "\n",
        "# Step 5: Optional Extension - Streamlit for Deployment\n",
        "\n",
        "# Streamlit app for next-word prediction\n",
        "def run_streamlit():\n",
        "    st.title(\"Next-Word Prediction Model\")\n",
        "\n",
        "    # Input field to enter text\n",
        "    input_text = st.text_input(\"Enter text:\", \"\")\n",
        "\n",
        "    if input_text:\n",
        "        # Use the model to generate the next word prediction\n",
        "        next_word_predictor = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "        prediction = next_word_predictor(input_text, max_length=50, num_return_sequences=1)\n",
        "\n",
        "        # Display the generated text prediction\n",
        "        st.write(f\"Predicted text: {prediction[0]['generated_text']}\")\n",
        "\n",
        "# This will not run directly in Colab, but is the Streamlit code for deployment.\n"
      ]
    }
  ]
}